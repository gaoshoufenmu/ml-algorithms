### 决策树

输入特征为$$d$$ 维，基于特征$$x_i$$ 的阈值$$\theta_i$$的指示函数$$\Bbb I_{[x_i \lt \theta_i]}$$决定从当前节点移动到左子节点还是右子节点，这是对应分为两个子空间的情况，如果特征$$x_i$$ 的值是有限离散集合，比如$$\lbrace 0,1 \rbrace$$，那么使用判别函数$$\Bbb I_{[x_i = 1]}$$。

#### 算法

我们使用某种"增益"测量方法来决定每次使用哪个属性来划分能最大化降低错误率。这里的“增益”测量方法暂且不表，让我们先看看构造决策树的算法。

输入：训练集$$S_0 = \lbrace (\mathbf x_1, y_1), (\mathbf x2, y_2), ...,(\mathbf x_m, y_m) \rbrace$$

属性集（下标）$$A_0 = \lbrace a_1, a_2, ..., a_d \rbrace$$

过程：函数$$TreeGenerate(S, A)$$

* 生成节点 $$node$$

* 如果$$S$$ 中所有样本属于同一分类$$C$$，则将$$node$$ 标记为$$C$$ 类叶节点；$$return$$

* 如果$$A$$ 为$$\varnothing$$ ，或者样本集$$S$$ 在属性集$$A$$ 上的各属性值分别相同（此时任何由增益选取的属性，都无法将当前样本空间划分为两个子空间，所有没必要再划分），将$$node$$ 标记为叶节点，其分类为$$S$$ 中样本数量最多的类；$$return$$

* $$ let j = argmax_{i \in A}  \ Gain(S, i)$$；

$$for$$ 属性 $$j$$ 的每一个值$$j_v$$ ，$$do$$

$$if \ S\\_v = \varnothing, \ then$$ 将$$node$$ 标记为叶节点，其分类为$$S$$ 中分类数量最多的类；$$return$$

else

以$$TreeGenerate\(S\_v, A- \lbrace j \rbrace\)$$为$$node$$ 的一个子节点

end if

$$end \ for$$

输出：以$$node$$ 为根节点的决策树

#### 增益度量

##### Gini Impurity

基尼不纯度表示从数据集S中随机选择两个样本，其类别不一样的概率，

$$Gini(S) = \sum_{i=1}^m  \sum_{j \neq i} p_i p_j = 1 - \sum_{i=1}^m p_i^2$$

在这里，就是指随机选取一个样本点，对样本误分类的概率。

属性$$j$$ 的基尼指数为，

$$GiniIndex(S, j) = \sum_{v=1}^V \frac {|S_v|} {|S|} Gini(S_v)$$

所以优先选择属性集合$$A$$ 中使得划分后基尼指数最小的属性，也就是说，使用这样的属性划分后，各个样本子集的混乱程度最小。

