### 决策树

输入特征为$$d$$ 维，基于特征$$x_i$$ 的阈值$$\theta_i$$的指示函数$$\Bbb I_{[x_i \lt \theta_i]}$$决定从当前节点移动到左子节点还是右子节点，这是对应分为两个子空间的情况，如果特征$$x_i$$ 的值是有限离散集合，比如$$\lbrace 0,1 \rbrace$$，那么使用判别函数$$\Bbb I_{[x_i = 1]}$$。

#### 算法

我们使用某种"增益"测量方法来决定每次使用哪个属性来划分能最大化降低错误率。这里的“增益”测量方法暂且不表，让我们先看看构造决策树的算法。

输入：训练集$$S_0 = \lbrace (\mathbf x_1, y_1), (\mathbf x_2, y_2), ...,(\mathbf x_m, y_m) \rbrace$$

属性集（下标）$$A_0 = \lbrace a_1, a_2, ..., a_d \rbrace$$

过程：函数$$TreeGenerate(S, A)$$

* 生成节点 $$node$$

* 如果$$S$$ 中所有样本属于同一分类$$C$$，则将$$node$$ 标记为$$C$$ 类叶节点；$$return$$

* 如果$$A$$ 为$$\varnothing$$ ，或者样本集$$S$$ 在属性集$$A$$ 上的各属性值分别相同（此时任何由增益选取的属性，都无法将当前样本空间划分为两个子空间，所有没必要再划分），将$$node$$ 标记为叶节点，其分类为$$S$$ 中样本数量最多的类；$$return$$

* $$ let \ j = argmax_{i \in A}  \ Gain(S, i)$$；

$$for$$ 属性 $$j$$ 的每一个值$$j_v$$ ，$$do$$

为$$node$$ 生成一个分支；令$$S_v$$ 为$$S$$ 中属性j 的值为$$j_v$$ 的样本子集；

$$if \ S_v = \varnothing, \ then$$ 将$$node$$ 标记为叶节点，其分类为$$S$$ 中分类数量最多的类（因为样本集没有覆盖到）；$$return$$

$$else$$

以$$TreeGenerate(S_v, A- \lbrace j \rbrace)$$为$$node$$ 的一个子节点

$$end \ if$$

$$end \ for$$

输出：以$$node$$ 为根节点的决策树

#### 增益度量

##### Gini Impurity

基尼不纯度表示从数据集S中随机选择两个样本，其类别不一样的概率，

$$Gini(S) = \sum_{i=1}^m  \sum_{j \neq i} p_i p_j = 1 - \sum_{i=1}^m p_i^2$$

在这里，就是指随机选取一个样本点，对样本误分类的概率。

属性$$j$$ 的基尼指数为，

$$GiniIndex(S, j) = \sum_{v=1}^V \frac {|S_v|} {|S|} Gini(S_v)$$

所以优先选择属性集合$$A$$ 中使得划分后基尼指数最小的属性，也就是说，使用这样的属性划分后，各个样本子集的混乱程度最小。

##### 信息增益

\* 信息熵

信息熵是随机变量不确定性的度量，熵越大，随机变量越不确定。

设离散随机变量$$X$$，概率分布为，

$$P\(X = x\_i\) = p\_i, \quad i = 1,2,...,n

则$$X$$的熵定义为，

$$H\(X\) = - \sum\_{i=1}^n p\_i log p\_i $$

\* 条件熵

二维离散随机变量$$\(X, Y\)$$，联合概率分布为，

$$P\(X = x\_i, Y = y\_j\) = p\_{ij}, \quad i = 1,2...,n; \ j = 1,2,...,m$$

则条件熵$$H\(Y\|X\)表示在已知随机变量$$X$$的条件下随机变量$$Y$$的不确定性，$$H\(Y\|X\)$$ 定义为已知$$X$$的条件下$$Y$$的条件概率分布的熵对$$X$$的期望，

$$H\(Y\|X\) = \sum\_{i=1} p\_i H\(Y\|X=x\_i\)$$

而依葫芦画瓢，下面的推导不难，

$$H\(Y\|X = x\_i\) = - \sum\_{j=1}^m \(P\(Y = y\_j\|X = x\_i\) log P\(Y = y\_j\|X = x\_i\)\)$$

将$$P\(Y = y\_j\|X = x\_i\)$$ 简记为$$p\_{j\|i}$$，于是，

$$H\(Y\|X\) = -\sum\_{i=1}^n \sum\_{j=1}^m p\_i p\_{j\|i} log p\_{j\|i} = - \sum\_{i=1}^n \sum\_{j=1}^m p\_{ij} log p\_{j\|i}$$



实际计算中，我们通常使用数据估计得到熵和条件熵，分别表示经验熵和经验条件熵。



\* 信息增益

训练数据集记为$$D$$，与上文的样本集$$S$$表示同一个意思，如无特殊说明的话。

特征$$A$$对$$D$$的信息增益定义为，经验熵$$H\(D\)$$ 与经验条件熵$$H\(D\|A\)$$的差，

$$g\(D,A\) = H\(D\) - H\(D\|A\)$$

很显然，$$g\(D,A\)$$越大，按特征$$A$$切分后的各样本子集的熵越小，表示这个特征的分类越强，所以优先使用$$g\(D, A\)$$较大的特征来对数据集进行分类。



* 信息增益比

然而，优先使用信息增益较大的特征来划分数据集会偏向于选择那些特征值数量较多的特征，极端情况下，就是数据集中的数据存在一个特征，每个数据在这个特征上取值都是唯一的，那么信息增益会选择这个特征来划分数据集，使得每个分支上仅包含一个数据样本，使得每个分支自身纯度达到最大，然而这样的决策树不具有泛化能力，无法对新样本进行预测。一种解决方法是使用信息增益比，定义为，

$$g\_R\(D, A\) = p \cdot g\(D, A\)$$

其中$$p$$为惩罚参数，通常是将某个特征$$A$$看作随机变量，按照其特征取值对数据集$$D$$划分，计算熵$$H\_A\(D\)$$，由于对特征值数目较多的特征惩罚力度较大（因为其泛化能力不强），所以取熵$$H\_A\(D\)$$的倒数，于是，

$$g\_R\(D, A\) = \frac {g\(D, A\)} {H\_A\(D\)}$$

$$H\_A\(D\) = - \sum{i=1}^V \frac {\|D\_i\|} {\|D\|} log \frac {\|D\_i\|} {\|D\|} $$

其中，特征$$A$$的取值数目为$$V$$，$$D\_i$$为按特征值$$A\_i$$对数据集$$D$$划分的子集。

当然信息增益比偏向取值较少的特征，这又违背了前面分析的选择使得划分的数据子集纯度尽可能高的原则，所以实际中通常 筛选出信息增益高于平均水平的特征，然而再从中选择信息增益比最高的特征，这是一种折中方案。

