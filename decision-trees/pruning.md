决策树的学习，为了避免过拟合，需要进行剪枝。基本策略有“预剪枝”和“后剪枝”。

#### 预剪枝

在决策树生成的过程中，对每个节点先进行估计，若当前节点的划分并不能提高判断准确率，那么就不进行划分，而直接将节点标记为叶节点。

实际操作时，可以采用留出法，将数据集中一部分数据用作验证集。评估是否需要对当前节点划分，假设节点对应的样本集为$$S_{p}$$，其分类为样本集中数量最多的那个分类，记为$$c$$，然后验证集在此节点上的数据集记为$$S_{vp}$$，从而划分前的准确率为$$\frac {|S_{vp}(C = c)|} {|S_{vp}|}$$，根据决策树生成规则，假设当前节点按某个属性$$A$$的值$$a_i$$划分为$$n$$个子分支（$$A$$取值数目为$$n$$），每个子分支的对应的样本子集为$$S_{i}$$，且有$$\sum_{i=1}^n S_{i} = S_{p}$$，对每个子分支，其分类暂时使用对应样本子集$$S_{i}$$上数量最多的那个分类，假设为$$c_i$$，那么划分后的准确率为$$\frac {\sum_{i=1}^n |S_{vp}(C = c_i, A = a_i)|} {|S_{vp}|}$$，比较这两个准确率，如果准确率不提高，则不划分当前节点。

虽然根据对当前节点是否能提高判断准确率来评估是否需要划分，解决了过拟合的问题，但是却无法预料当前节点的子节点的后续划分是否能显著提高准确率，所以一定程度上引入了欠拟合的风险。

#### 后剪枝

在决策树生成完成后进行剪枝。

自下而上的遍历非叶节点，比如对当前非叶节点，如果将其分支剪除，作为叶节点处理，那么分类为样本空间中数目最多的那个分类，使用验证集来计算整个决策树的准确率，若与计算前的决策树准确率相比没有降低，则进行剪枝。

